{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Training and Inference\n",
    "\n",
    "## 0. Datasets and DataLoader class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are classes that access data samples. They require 2 methods:\n",
    "- \\_\\_len\\_\\_: return the total numbers of samples\n",
    "- \\_\\_getitem\\_\\_: retrieve a specific sample by index\n",
    "\n",
    "DataLoaders are classes that handles data procesing fro training. They deal with tasks such as:\n",
    "- create batches\n",
    "- shuffle data\n",
    "- parallel processing (loading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw implementation of custom data class\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data = np.load(data_path)\n",
    "        #users can actually put data processing pipeline here\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data(index)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are already pre-built Dataset classes to handle common datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular tensors (already in memory)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "X = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float)\n",
    "y = torch.tensor([0, 1, 0], dtype=torch.float)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([ #tensor processing pipeline\n",
    "    transforms.Resize(256), #resizes image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.75, 0.75, 0.75])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"../../../data/random_images\", transform=transform)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 additional features can be used to customize DataLoaders:\n",
    "1. Sampling methods\n",
    "    - entered as `sampler` argument in declaration\n",
    "2. Collate functions (how samples are combined into a batch)\n",
    "    - This cannot be used with shuffle\n",
    "    - entered as `collate_fn` argument in declaration\n",
    "\n",
    "Collate functions take in `batch` as arguments and returns `padded_sequences`, `labels`, and `sequence_lengths`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model architecture\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, #pictures are greyscale\n",
    "            out_channels=32, \n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, \n",
    "            out_channels=64, \n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128) # (28 - 2 - 2)^2 * 64\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #layer 1\n",
    "        x = F.relu(x) #activation function 1\n",
    "        x = self.conv2(x) #layer 2\n",
    "        x = F.relu(x) #activation function 2\n",
    "        x = F.max_pool2d(x, 2) #layer 2 - weed out dimensions\n",
    "        x = self.dropout1(x) #layer 2 - ease overfitting\n",
    "        x = torch.flatten(x, 1) #transform to 1-d array\n",
    "        x = self.fc1(x) #layer 3\n",
    "        x = F.relu(x) #layer 3 - activation function\n",
    "        x = self.dropout2(x) #layer 3 - ease overfitting\n",
    "        x = self.fc2(x) #layer 4\n",
    "        output = F.log_softmax(x, dim = 1) #layer 4 - final result\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300810\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.665348\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.410099\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.294839\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.190624\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.181525\n",
      "\n",
      "Test set: Average loss: 0.0862, Accuracy: 9748/10000 (97%)\n",
      "\n",
      "Train Epoch: 2\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.140380\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.132613\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.120765\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.121433\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.103166\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.105827\n",
      "\n",
      "Test set: Average loss: 0.0530, Accuracy: 9823/10000 (98%)\n",
      "\n",
      "Train Epoch: 3\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.135946\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.083044\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.065545\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.074221\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.113892\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.075942\n",
      "\n",
      "Test set: Average loss: 0.0407, Accuracy: 9865/10000 (99%)\n",
      "\n",
      "Train Epoch: 4\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.052985\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.067985\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.060081\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.062812\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.103689\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.051059\n",
      "\n",
      "Test set: Average loss: 0.0351, Accuracy: 9874/10000 (99%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.061887\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.078801\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.049536\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.056977\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.065725\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.032532\n",
      "\n",
      "Test set: Average loss: 0.0328, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.064567\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.043386\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.070615\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.069559\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.041997\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.038205\n",
      "\n",
      "Test set: Average loss: 0.0293, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 7\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.052867\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.031755\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.027035\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.034523\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.029314\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.034792\n",
      "\n",
      "Test set: Average loss: 0.0308, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 8\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.040792\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.028815\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.041857\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.022935\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.054453\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.033759\n",
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.029062\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.037684\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.042613\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.042047\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.036820\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.039813\n",
      "\n",
      "Test set: Average loss: 0.0288, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.036855\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.019514\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.036038\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.035741\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.029619\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.046478\n",
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Train Epoch: 11\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.021557\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 0.027606\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.041052\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 0.018387\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.038301\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 0.035657\n",
      "\n",
      "Test set: Average loss: 0.0268, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Train Epoch: 12\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.044245\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 0.023481\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.029714\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 0.051850\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.019491\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 0.028172\n",
      "\n",
      "Test set: Average loss: 0.0258, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train Epoch: 13\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.021930\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 0.033132\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.024785\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 0.014434\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.054942\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 0.048844\n",
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Train Epoch: 14\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.026029\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 0.017605\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.041420\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 0.025332\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.028719\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 0.028969\n",
      "\n",
      "Test set: Average loss: 0.0272, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Train Epoch: 15\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.026752\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 0.018976\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.025954\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 0.042830\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.021669\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 0.027934\n",
      "\n",
      "Test set: Average loss: 0.0259, Accuracy: 9927/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# create model instance and load MNIST data\n",
    "model = CNN()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.75))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    \"../../../data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    \"../../../data\", \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=1000, \n",
    "    shuffle=True\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=1000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# model training\n",
    "device = torch.device(\"cpu\") # train on cpu\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) #different optimizers may have significantly different results\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Train Epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for batch_id, (data, target) in enumerate(train_data_loader): #for each batch, also get the index of batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) #forward pass\n",
    "        loss = F.nll_loss(output, target) #negative loglikelihood loss function\n",
    "        loss.backward() #compute gradients\n",
    "        optimizer.step() #update weights\n",
    "\n",
    "        if batch_id % 20 == 0: #update on training iterations\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id * len(data), len(train_data_loader.dataset),\n",
    "                100. * batch_id / len(train_data_loader), loss.item()))\n",
    "    \n",
    "    model.eval() #test set\n",
    "    test_loss = 0 #0 out losses\n",
    "    correct = 0\n",
    "    with torch.no_grad(): #disable gradient calculation\n",
    "        for data, target in test_data_loader: #for each batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data) #forward pass\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_data_loader.dataset)\n",
    "\n",
    "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss, correct, len(test_data_loader.dataset),\n",
    "        100. * correct / len(test_data_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "torch.save(model.state_dict(), \"./models/MNIST_CNN_weights.pt\") #saves model weights\n",
    "torch.save(model, \"./models/MNIST_CNN_model.pt\") #not recommended for production as it would break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model_state_reloaded = CNN() #initialize the instance \n",
    "model_state_reloaded.load_state_dict( #restore the weights\n",
    "    torch.load(\"./models/MNIST_CNN_weights.pt\", weights_only=True)\n",
    ")\n",
    "\n",
    "model_reloaded = torch.load(\"./models/MNIST_CNN_model.pt\", weights_only=False) #loads the whole saved instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Inferencing on CNN\n",
    "\n",
    "The following steps are for inferencing an existing trained model. Most of it is already implemented in the sample training code.\n",
    "\n",
    "0. Load the model and weights (only if necessary)\n",
    "1. Set model in eval mode `model.eval()`\n",
    "2. Conduct forward pass with input `raw_output = model(input_data)` and ensure no gradients are calculated along the way `with torch.no_grad():`\n",
    "3. Transform and interpret raw_output for use cases as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
