{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Optimizers\n",
    "\n",
    "PyTorch optimizers essentially are in charge of updating the weights with each iteration of the training loop. How they update the weights in each iteration depends on the optimization algorithm (ex. stochastic gradient descent). Optimizers should be used over manual gradient updates (most of the time) as it is more adaptive and may be able to train the model faster (with less iterations) with less errors.\n",
    "\n",
    "PyTorch comes with the following optimizers out of the box:\n",
    "- SGD (stochastic gradient descent)\n",
    "- Adam (adaptive moment estimation)\n",
    "- RMSprop\n",
    "- Adagrad\n",
    "- AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: pre-update x = 2.0263\n",
      "Iteration 20: post-update x = 1.8236, \n",
      "resulting y = 4.1058\n",
      "Iteration 40: pre-update x = 0.2463\n",
      "Iteration 40: post-update x = 0.2217, \n",
      "resulting y = 0.0607\n",
      "Iteration 60: pre-update x = 0.0300\n",
      "Iteration 60: post-update x = 0.0270, \n",
      "resulting y = 0.0009\n",
      "Iteration 80: pre-update x = 0.0036\n",
      "Iteration 80: post-update x = 0.0033, \n",
      "resulting y = 0.0000\n",
      "Iteration 100: pre-update x = 0.0004\n",
      "Iteration 100: post-update x = 0.0004, \n",
      "resulting y = 0.0000\n",
      "[15.0, 1.823649525642395, 0.22171321511268616, 0.026955142617225647, 0.0032771159894764423, 0.0003984206705354154]\n",
      "[225.0, 3.3256975923757324, 0.049156749755604245, 0.0007265797135149743, 1.0739489208482162e-05, 1.5873903070989003e-07]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "x = torch.tensor([15], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([x], lr=0.05) #stochastic gradient descent with learning rate = 0.05\n",
    "\n",
    "x_iters = [x.item()]\n",
    "y_hist = [x.item() ** 2]\n",
    "\n",
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    optimizer.zero_grad() #clear accumulated gradient calculations\n",
    "\n",
    "    y = x ** 2 # forward pass to calculate function output\n",
    "\n",
    "    y.backward() # calculate gradient value at x\n",
    "\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Iteration {i+1}: pre-update x = {x.item():.4f}\")\n",
    "\n",
    "    optimizer.step() # updates the actual x to the argument closer\n",
    "\n",
    "    if (i + 1) % 20 == 0:\n",
    "        x_iters.append(x.item())\n",
    "        y_hist.append(x.item() ** 2)\n",
    "        print(f\"Iteration {i+1}: post-update x = {x.item():.4f}, \\nresulting y = {y.item():.4f}\")\n",
    "\n",
    "print(x_iters)\n",
    "print(y_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the following are steps to how to use existing optimizers in PyTorch:\n",
    "\n",
    "1. Initialize optimizer object with function import variable (ex. weights to a loss function) and other hyperparameters (ex. learning rate, momentum etc.)\n",
    "    - Note that different optimization algorithms require different hyperparameters.\n",
    "2. Start optimization loop\n",
    "3. Zero out existing gradients with `optimizer.zero_grad()`\n",
    "4. Forward pass (ie. calculate function output with inputs)\n",
    "5. Backward pass (ie. calculate gradient value evaluated at current inputs): `y.backward()`\n",
    "6. Update what the next input value should be with `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "100\n",
      "torch.float64\n",
      "torch.Size([100, 2])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# try in actual learning cases\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "weight = [0.77, -0.56]\n",
    "bias = np.random.normal(0,12)\n",
    "SEED = 9999\n",
    "\n",
    "X = np.random.rand(100, 2) * 10\n",
    "y = X @ weight + bias\n",
    "\n",
    "print(X.size)\n",
    "print(y.size)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "print(X.dtype)\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([-1.7755,  1.4201], dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([1.0382], dtype=torch.float64, requires_grad=True)]\n",
      "OrderedDict([('weights', tensor([-1.7755,  1.4201], dtype=torch.float64)), ('bias', tensor([1.0382], dtype=torch.float64))])\n",
      "Epoch: 1 \n",
      " Weights: Parameter containing:\n",
      "tensor([-1.2778,  1.5626], dtype=torch.float64, requires_grad=True) \n",
      " Training Loss: 13091.3530\n",
      "Epoch: 101 \n",
      " Weights: Parameter containing:\n",
      "tensor([ 0.9495, -0.3987], dtype=torch.float64, requires_grad=True) \n",
      " Training Loss: 34.4812\n",
      "Epoch: 201 \n",
      " Weights: Parameter containing:\n",
      "tensor([ 0.8271, -0.5016], dtype=torch.float64, requires_grad=True) \n",
      " Training Loss: 3.8853\n",
      "Epoch: 301 \n",
      " Weights: Parameter containing:\n",
      "tensor([ 0.7892, -0.5404], dtype=torch.float64, requires_grad=True) \n",
      " Training Loss: 0.4403\n",
      "Epoch: 401 \n",
      " Weights: Parameter containing:\n",
      "tensor([ 0.7765, -0.5534], dtype=torch.float64, requires_grad=True) \n",
      " Training Loss: 0.0499\n",
      "final weights: Parameter containing:\n",
      "tensor([ 0.7722, -0.5577], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# simple linear regression learning\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class OLS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #initialize weights with a random vector\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(\n",
    "                2,\n",
    "                requires_grad=True, #PyTorch will track gradients of this param\n",
    "                dtype=torch.float64\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #initialize bias with a random scalar\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1,\n",
    "                requires_grad=True, #PyTorch will track gradients of this param\n",
    "                dtype=torch.float64\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "model = OLS() #initialize class\n",
    "print(list(model.parameters())) #checks current state of model (prior to training)\n",
    "print(model.state_dict())\n",
    "\n",
    "X_train, X_validate, X_test = X[:80], X[80:90], X[90:]\n",
    "y_train, y_validate, y_test = y[:80], y[80:90], y[90:]\n",
    "\n",
    "iterations = 500 #this is the epoch in this case\n",
    "learning_rate = 0.00005\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9) #step 1\n",
    "\n",
    "for epoch in range(iterations): #step 2\n",
    "    optimizer.zero_grad() #step 3\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    y_pred = model(X_train) #step 4\n",
    "\n",
    "    loss = torch.sum((y_pred - y_train) ** 2)\n",
    "    loss.backward() #step 5\n",
    "\n",
    "    model.eval()\n",
    "    optimizer.step() #step 6\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch+1} \\n Weights: {model.weights} \\n Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"final weights: {model.weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
