{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Training and Inference\n",
    "\n",
    "## 0. Datasets and DataLoader class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are classes that access data samples. They require 2 methods:\n",
    "- \\_\\_len\\_\\_: return the total numbers of samples\n",
    "- \\_\\_getitem\\_\\_: retrieve a specific sample by index\n",
    "\n",
    "DataLoaders are classes that handles data procesing fro training. They deal with tasks such as:\n",
    "- create batches\n",
    "- shuffle data\n",
    "- parallel processing (loading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw implementation of custom data class\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data = np.load(data_path)\n",
    "        #users can actually put data processing pipeline here\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data(index)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are already pre-built Dataset classes to handle common datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular tensors (already in memory)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "X = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float)\n",
    "y = torch.tensor([0, 1, 0], dtype=torch.float)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([ #tensor processing pipeline\n",
    "    transforms.Resize(256), #resizes image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.75, 0.75, 0.75])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"../../../data/random_images\", transform=transform)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 additional features can be used to customize DataLoaders:\n",
    "1. Sampling methods\n",
    "    - entered as `sampler` argument in declaration\n",
    "2. Collate functions (how samples are combined into a batch)\n",
    "    - This cannot be used with shuffle\n",
    "    - entered as `collate_fn` argument in declaration\n",
    "\n",
    "Collate functions take in `batch` as arguments and returns `padded_sequences`, `labels`, and `sequence_lengths`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a basic feedforward neural network\n",
    "\n",
    "Build a simple 3 layer (1 hidden layer) neural network on MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a model architecture\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) #weights to go to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim) #weights to go to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        first_layer = self.fc1(x)\n",
    "        activated_first_layer = torch.tanh(first_layer) #use tanh as activation function for 1st layer\n",
    "        second_layer = self.fc2(activated_first_layer)\n",
    "        output = torch.sigmoid(second_layer) #use sigmoid as activation function for 2nd layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306831\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.112447\n",
      "Evaluate Epoch: 1\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 7601/10000 (76%)\n",
      "\n",
      "Train Epoch: 2\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.078799\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.963991\n",
      "Evaluate Epoch: 2\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 8096/10000 (81%)\n",
      "\n",
      "Train Epoch: 3\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.944077\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.875516\n",
      "Evaluate Epoch: 3\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 8353/10000 (84%)\n",
      "\n",
      "Train Epoch: 4\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.864811\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.823008\n",
      "Evaluate Epoch: 4\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 8500/10000 (85%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.812192\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.777472\n",
      "Evaluate Epoch: 5\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 8594/10000 (86%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.764850\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.739041\n",
      "Evaluate Epoch: 6\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 8711/10000 (87%)\n",
      "\n",
      "Train Epoch: 7\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.730593\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.711599\n",
      "Evaluate Epoch: 7\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 8780/10000 (88%)\n",
      "\n",
      "Train Epoch: 8\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.704188\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.688841\n",
      "Evaluate Epoch: 8\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 8844/10000 (88%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.687309\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.669916\n",
      "Evaluate Epoch: 9\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 8898/10000 (89%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.671151\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.655422\n",
      "Evaluate Epoch: 10\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 8946/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# create model instance and load MNIST data\n",
    "model = FeedForwardNN(\n",
    "    input_dim=28*28, #image size\n",
    "    hidden_dim=100, #hidden nodes\n",
    "    output_dim=10 #10 categories as output\n",
    ")\n",
    "\n",
    "batch_size = 5000\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    \"../../../data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    \"../../../data\", \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# model training\n",
    "device = torch.device(\"cpu\") # train on cpu\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) #different optimizers may have significantly different results\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Train Epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for batch_id, (data, target) in enumerate(train_data_loader): #for each batch, also get the index of batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, 28*28).requires_grad_()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) #forward pass\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, target) #cross entropy loss as loss function\n",
    "        loss.backward() #compute gradients\n",
    "        optimizer.step() #update weights\n",
    "\n",
    "        if batch_id % 10 == 0: #update on training iterations\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id * len(data), len(train_data_loader.dataset),\n",
    "                100. * batch_id / len(train_data_loader), loss.item()))\n",
    "    \n",
    "    model.eval() #test set\n",
    "    test_loss = 0 #0 out losses\n",
    "    correct = 0\n",
    "    print(f\"Evaluate Epoch: {epoch}\")\n",
    "    with torch.no_grad(): #disable gradient calculation\n",
    "        for data, target in test_data_loader: #for each batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 28*28)\n",
    "            output = model(data) #forward pass\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            test_loss += criterion(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_data_loader.dataset)\n",
    "\n",
    "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss, correct, len(test_data_loader.dataset),\n",
    "        100. * correct / len(test_data_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "torch.save(model.state_dict(), \"./models/MNIST_feedforward_weights.pt\") #saves model weights\n",
    "torch.save(model, \"./models/MNIST_feedforward_model.pt\") #not recommended for production as it would break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model_state_reloaded = FeedForwardNN( #must use the same parameters\n",
    "    input_dim=28*28,\n",
    "    hidden_dim=100, \n",
    "    output_dim=10\n",
    ") #initialize the instance \n",
    "model_state_reloaded.load_state_dict( #restore the weights\n",
    "    torch.load(\"./models/MNIST_feedforward_weights.pt\", weights_only=True)\n",
    ")\n",
    "\n",
    "model_reloaded = torch.load(\"./models/MNIST_feedforward_model.pt\", weights_only=False) #loads the whole saved instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Inferencing on Feedforward model\n",
    "\n",
    "The following steps are for inferencing an existing trained model. Most of it is already implemented in the sample training code.\n",
    "\n",
    "0. Load the model and weights (if necessary)\n",
    "1. Set model in eval mode `model.eval()`\n",
    "2. Conduct forward pass with input `raw_output = model(input_data)` and ensure no gradients are calculated along the way `with torch.no_grad():`\n",
    "3. Transform and interpret raw_output for use cases as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
