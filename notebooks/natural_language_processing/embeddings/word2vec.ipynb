{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d72d76",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "To simplify, we will train the following small corpus from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab0cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words (including punctuation): 147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fellow-Citizens of the Senate and of the House of Representatives:\\n\\nAmong the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years -- a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but overwhelm with despondence one who (inheriting inferior endowments from nature and unpracticed in the duties of civil administration) ought to be peculiarly conscious of his own deficiencies. In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected. All I dare hope is that if, in executing this task, I have been too much swayed by a grateful remembrance of former instances, or by an affectionate sensibility to this transcendent proof of the confidence of my fellow citizens, and have thence too little consulted my incapacity as well as disinclination for the weighty and untried cares before me, my error will be palliated by the motives which mislead me, and its consequences be judged by my country with some share of the partiality in which they originated.\\n\\nSuch being the impressions under which I have, in obedience to the public summons, repaired to the present station, it would be peculiarly improper to omit in this first official act my fervent supplications to that Almighty Being who rules over the universe, who presides in the councils of nations, and whose providential aids can supply every human defect, that His benediction may consecrate to the liberties and happiness of the people of the United States a Government instituted by themselves for these essential purposes, and may enable every instrument employed in its administration to execute with success the functions allotted to his charge. In tendering this homage to the Great Author of every public and private good, I assure myself that it expresses your sentiments not less than my own, nor those of my fellow citizens at large less than either. No people can be bound to acknowledge and adore the Invisible Hand which conducts the affairs of men more than those of the United States. Every step by which they have advanced to the character of an independent nation seems to have been distinguished by some token of providential agency; and in the important revolution just accomplished in the system of their united government the tranquil deliberations and voluntary consent of so many distinct communities from which the event has resulted can not be compared with the means by which most governments have been established without some return of pious gratitude, along with an humble anticipation of the future blessings which the past seem to presage. These reflections, arising out of the present crisis, have forced themselves too strongly on my mind to be suppressed. You will join with me, I trust, in thinking that there are none under the influence of which the proceedings of a new and free government can more auspiciously commence.\\n\\nBy the article establishing the executive department it is made the duty of the President \"to recommend to your consideration such measures as he shall judge necessary and expedient.\" The circumstances under which I now meet you will acquit me from entering into that subject further than to refer to the great constitutional charter under which you are assembled, and which, in defining your powers, designates the objects to which your attention is to be given. It will be more consistent with those circumstances, and far more congenial with the feelings which actuate me, to substitute, in place of a recommendation of particular measures, the tribute that is due to the talents, the rectitude, and the patriotism which adorn the characters selected to devise and adopt them. In these honorable qualifications I behold the surest pledges that as on one side no local prejudices or attachments, no separate views nor party animosities, will misdirect the comprehensive and equal eye which ought to watch over this great assemblage of communities and interests, so, on another, that the foundation of our national policy will be laid in the pure and immutable principles of private morality, and the preeminence of free government be exemplified by all the attributes which can win the affections of its citizens and command the respect of the world. I dwell on this prospect with every satisfaction which an ardent love for my country can inspire, since there is no truth more thoroughly established than that there exists in the economy and course of nature an indissoluble union between virtue and happiness; between duty and advantage; between the genuine maxims of an honest and magnanimous policy and the solid rewards of public prosperity and felicity; since we ought to be no less persuaded that the propitious smiles of Heaven can never be expected on a nation that disregards the eternal rules of order and right which Heaven itself has ordained; and since the preservation of the sacred fire of liberty and the destiny of the republican model of government are justly considered, perhaps, as deeply, as finally, staked on the experiment entrusted to the hands of the American people.\\n\\nBesides the ordinary objects submitted to your care, it will remain with your judgment to decide how far an exercise of the occasional power delegated by the fifth article of the Constitution is rendered expedient at the present juncture by the nature of objections which have been urged against the system, or by the degree of inquietude which has given birth to them. Instead of undertaking particular recommendations on this subject, in which I could be guided by no lights derived from official opportunities, I shall again give way to my entire confidence in your discernment and pursuit of the public good; for I assure myself that whilst you carefully avoid every alteration which might endanger the benefits of an united and effective government, or which ought to await the future lessons of experience, a reverence for the characteristic rights of freemen and a regard for the public harmony will sufficiently influence your deliberations on the question how far the former can be impregnably fortified or the latter be safely and advantageously promoted.\\n\\nTo the foregoing observations I have one to add, which will be most properly addressed to the House of Representatives. It concerns myself, and will therefore be as brief as possible. When I was first honored with a call into the service of my country, then on the eve of an arduous struggle for its liberties, the light in which I contemplated my duty required that I should renounce every pecuniary compensation. From this resolution I have in no instance departed; and being still under the impressions which produced it, I must decline as inapplicable to myself any share in the personal emoluments which may be indispensably included in a permanent provision for the executive department, and must accordingly pray that the pecuniary estimates for the station in which I am placed may during my continuance in it be limited to such actual expenditures as the public good may be thought to require.\\n\\nHaving thus imparted to you my sentiments as they have been awakened by the occasion which brings us together, I shall take my present leave; but not without resorting once more to the benign Parent of the Human Race in humble supplication that, since He has been pleased to favor the American people with opportunities for deliberating in perfect tranquillity, and dispositions for deciding with unparalleled unanimity on a form of government for the security of their union and the advancement of their happiness, so His divine blessing may be equally conspicuous in the enlarged views, the temperate consultations, and the wise measures on which the success of this Government must depend. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "\n",
    "raw_corpus = inaugural.raw(\"1789-Washington.txt\")\n",
    "print(f\"words (including punctuation): {len(inaugural.words('1793-Washington.txt'))}\")\n",
    "raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe58cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d074dd7",
   "metadata": {},
   "source": [
    "## Aside: understanding `nn.Embedding` in PyTorch\n",
    "\n",
    "`nn.Embedding` generates a $n\\times m$ matrix where n = number of words (ie. vocabulary) and m = dimension of each word embedding. The creation of a `nn.Embedding` object will randomly initialize its entries (user can specify distribution to draw from) in model class relation.\n",
    "\n",
    "A forward pass to a `nn.Embedding` object takes a torch tensor of `torch.int64` data type (ie. `torch.LongTensor` type) and returns the corresponding indicies row of embeddings to the integer entries of the input. (ex. [0] will return the 1st embedding vector, see examples in cells below). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ca6cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.5434, -2.1433, -0.5919],\n",
      "        [ 0.3199,  0.3632,  1.5093],\n",
      "        [-1.1995, -0.9635, -0.2396]], requires_grad=True)\n",
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5434, -2.1433, -0.5919],\n",
       "        [ 0.3199,  0.3632,  1.5093]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(3, 3) # create a 3x3 matrix of embeddings\n",
    "print(embedding.weight)\n",
    "input = torch.tensor([0, 1], dtype=torch.int64) # returns the 1st and 2nd ROW of the matrix\n",
    "print(input.dtype)\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f85421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2629,  1.2739,  1.8622],\n",
      "        [ 1.0560,  0.5023,  0.9591],\n",
      "        [-1.5599, -0.6480,  0.1509],\n",
      "        [-1.6602, -0.4377,  0.8469],\n",
      "        [-0.3645,  1.2964,  0.8005]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2629,  1.2739,  1.8622],\n",
       "         [ 1.0560,  0.5023,  0.9591]],\n",
       "\n",
       "        [[-1.6602, -0.4377,  0.8469],\n",
       "         [-0.3645,  1.2964,  0.8005]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(5, 3) # create a 5x3 matrix of embeddings\n",
    "print(embedding.weight)\n",
    "input = torch.tensor([[0, 1], [3,4]], dtype=torch.long) \n",
    "# returns 3rd order tensor with first entry in 1st dimension = 1st and 2nd row embedding vectors and second entry = 4th and 5th row embedding vectors\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f61cc",
   "metadata": {},
   "source": [
    "## Skip-gram architecture\n",
    "\n",
    "### Model inputs\n",
    "\n",
    "Skip-gram word2vec takes in a pair of integers that represent a word pairing between a \"center word\" and an \"outside word\" the integers are usually the index of the word (or token) in the vocabulary (list object of words/token the model is to be trained on). The center word here is self-explanatory. What determines if the word should be outside is based on the `window` parameter of the model where it determines how many slots are to be considered (usually 2-4 slots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c992682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset class to handle corpus and feeding dataset for training + inference (ie. generate integer pairs from a corpus)\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        corpus: str, \n",
    "        window_size: int = 2,\n",
    "        min_count: int = 5 # required minimum times appeared by the word in corpus to be incorporated into vocabulary\n",
    "    ):\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.tokens = self._tokenize(corpus)\n",
    "        self.vocab, self.word2idx, self.idx2word = self._build_vocab(min_count)\n",
    "        self.pairs = self._generate_pairs()\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        target = self.pairs[index][\"target\"]\n",
    "        context = self.pairs[index][\"context\"]\n",
    "        return torch.tensor([target], dtype=torch.long), torch.tensor([context], dtype=torch.long)\n",
    "    \n",
    "    def _tokenize(self, corpus: str) -> list[str]:\n",
    "        tokens = corpus.lower().split()\n",
    "        return tokens\n",
    "    \n",
    "    def _build_vocab(self, min_count: int) -> tuple[list[str], dict[str, int], dict[int, str]]:\n",
    "        \"\"\"Build vocabulary from tokens where it takes only words that occur more than minimal count and indices them in order of appearance\"\"\"\n",
    "        word_counts = Counter(self.tokens)\n",
    "        vocab = [word for word, count in word_counts.items() if count >= min_count]\n",
    "        word_2_index_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "        index_2_word_dict = {idx: word for word, idx in word_2_index_dict.items()}\n",
    "        return vocab, word_2_index_dict, index_2_word_dict\n",
    "    \n",
    "    def _generate_pairs(self) -> list[dict[str, int]]:\n",
    "        \"\"\"\n",
    "            For each word i in the corpus which are in the vocabulary:\n",
    "                create a pair where:\n",
    "                    1. target: index of the word i\n",
    "                    2. context: index of nth closest word (that is in the vocabulary) to i (where n is dictated by window_size)\n",
    "                append pair to pair list\n",
    "                repeat for same target with (n - 1)th closest applicable word \n",
    "                    and onwards until window size exhausted\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        indexed_tokens = [self.word2idx[token] for token in self.tokens if token in self.vocab]\n",
    "\n",
    "        for i, target_index in enumerate(indexed_tokens):\n",
    "            #Get context window\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(indexed_tokens), i + self.window_size + 1)\n",
    "\n",
    "            context_indices = [indexed_tokens[j] for j in range(start, end) if j != i]\n",
    "\n",
    "            # Skip-gram: predict context from target\n",
    "            for context_index in context_indices:\n",
    "                pairs.append({\n",
    "                    \"target\": target_index, \n",
    "                    \"context\": context_index\n",
    "                })\n",
    "\n",
    "        return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63622a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dataset = SkipGramDataset(raw_corpus)\n",
    "corpus_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472fa85",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638489cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(L.LightningModule):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        self.u_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Initialize with small random values\n",
    "        self.v_embeddings.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "        self.u_embeddings.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "\n",
    "        # Hyperparamters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, center_word: torch.Tensor, outside_word: torch.Tensor) -> torch.Tensor:\n",
    "        v_c = self.v_embeddings(center_word) #the embeddings are essentially row vectors\n",
    "        u_o = self.u_embeddings(outside_word) #the embeddings are essentially row vectors\n",
    "\n",
    "        numerator = torch.exp((u_o @ v_c.transpose(1, 2)).squeeze(2)) \n",
    "        lower_product = torch.exp((self.u_embeddings.weight.data @ v_c.transpose(1, 2)).squeeze(2)) \n",
    "        denominator = torch.sum(lower_product)\n",
    "\n",
    "        probability = numerator / denominator\n",
    "\n",
    "        return probability\n",
    "    \n",
    "    def loss(self, center_word: torch.Tensor, outside_word: torch.Tensor) -> torch.Tensor:\n",
    "        _probability = self.forward(center_word, outside_word)\n",
    "        loss = -torch.mean(torch.log(_probability).unsqueeze(1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch: tuple[torch.Tensor], batch_idx: int):\n",
    "        _input_target, _input_context = batch\n",
    "        training_loss = self.loss(_input_target, _input_context)\n",
    "        return training_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc6e6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0251]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test some outputs\n",
    "model = SkipGram(len(corpus_dataset.vocab), 10, learning_rate=0.001)\n",
    "output = model(torch.LongTensor([[0]]), torch.LongTensor([[2]]))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d5f0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6865, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model.loss(torch.LongTensor([[0]]), torch.LongTensor([[2]]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f165f4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffef43ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/tony/anaconda3/envs/rec-sys/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name         | Type      | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | v_embeddings | Embedding | 400    | train\n",
      "1 | u_embeddings | Embedding | 400    | train\n",
      "---------------------------------------------------\n",
      "800       Trainable params\n",
      "0         Non-trainable params\n",
      "800       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/tony/anaconda3/envs/rec-sys/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 629.27it/s, v_num=32]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 279/279 [00:00<00:00, 625.73it/s, v_num=32]\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 200\n",
    "batch_size = 10\n",
    "\n",
    "model = SkipGram(len(corpus_dataset.vocab), 10, learning_rate=0.001)\n",
    "trainer = L.Trainer(max_epochs=200)\n",
    "\n",
    "dataloader = DataLoader(corpus_dataset, batch_size=batch_size)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989f890",
   "metadata": {},
   "source": [
    "### Get embeddings for given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0641249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram_embedding(word: str, corpus: Dataset = corpus_dataset) -> torch.Tensor:\n",
    "    try:\n",
    "        idx = corpus.word2idx[word]\n",
    "    except KeyError:\n",
    "        raise Exception(f\"{word} is not part of the vocabulary\")\n",
    "    else:\n",
    "        _u = model.u_embeddings.weight.data[idx]\n",
    "        _v = model.v_embeddings.weight.data[idx]\n",
    "\n",
    "        embedding = (_u + _v) / 2 # the entries of the embedding vector for a given word is the mean of the u and v components\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed8facd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.0934,  0.6981,  1.0869,  5.4527,  4.2140, -3.8069,  4.3484,  3.7076,\n",
       "         1.7668, -2.1651])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get embedding for the word this:\n",
    "this = get_skipgram_embedding(\"this\")\n",
    "this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546c36c",
   "metadata": {},
   "source": [
    "## Continous Bag of Words (CBoW) architecture\n",
    "\n",
    "### Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ae98206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "class CBoWDataset:\n",
    "    def __init__(self, corpus: str, window_size: int = 2, min_word_count: int = 2):\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.min_freq = min_word_count\n",
    "        self.data = self._create_training_data()\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        target = self.data[index][\"target\"]\n",
    "        context = self.data[index][\"context\"]\n",
    "        return torch.tensor([context], dtype=torch.long), torch.tensor([target], dtype=torch.long)\n",
    "    \n",
    "    def _tokenize(self, corpus: str) -> list[str]:\n",
    "        tokens = corpus.lower().split()\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def _create_training_data(self):\n",
    "        _tokenized_text = self._tokenize(self.corpus)\n",
    "        word_counts = Counter(_tokenized_text)\n",
    "        self.vocab = [word for word, count in word_counts.items() if count >= self.min_freq]\n",
    "        self.word_2_index = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.index_2_word  = {idx: word for word, idx in self.word_2_index.items()}\n",
    "\n",
    "        _indexed_tokens = [self.word_2_index[word] for word in _tokenized_text if word in self.vocab]\n",
    "\n",
    "        data = []\n",
    "        for i, target_index in enumerate(_indexed_tokens):\n",
    "            #skip or end iterations where there are not enough context words (ie. out of corpus)\n",
    "            if i < self.window_size:\n",
    "                continue\n",
    "            if i + self.window_size >= len(_indexed_tokens):\n",
    "                break\n",
    "\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = i + self.window_size + 1\n",
    "\n",
    "            context_indices = [_indexed_tokens[j] for j in range(start, end) if j != i]\n",
    "            data.append({\n",
    "                \"context\": context_indices,\n",
    "                \"target\": target_index\n",
    "            })\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def get_pytorch_dataset(self) -> TensorDataset: #TODO: better refine this class into a subclass of Pytorch Dataset class.\n",
    "        if self.data is None:\n",
    "            raise AttributeError(\"Dataset is empty in object\")\n",
    "        \n",
    "        context_tensor = torch.tensor([d[\"context\"] for d in self.data])\n",
    "        target_tensor = torch.tensor([d[\"target\"] for d in self.data])\n",
    "\n",
    "        return TensorDataset(context_tensor, target_tensor)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18ef17",
   "metadata": {},
   "source": [
    "A record of training dataset for CBoW would be indices of the neighboring words (context) and the index of the target word (target). Below is an example of what a record in training data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c843babc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 0, 1]]), tensor([2]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_dataset = CBoWDataset(raw_corpus)\n",
    "cbow_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9851d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2435b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoW(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 100, learning_rate: int = 0.001):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, context: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings = self.embeddings(context).mean(1).squeeze(1)\n",
    "        scores = self.linear(embeddings)\n",
    "        return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937154f",
   "metadata": {},
   "source": [
    "Below is an example of what an output of the model would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e179e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 134])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3612, -0.0851,  0.1617, -0.4738, -0.7177,  0.4703, -0.4646,  0.0479,\n",
       "          0.2783,  0.5770, -0.0645,  0.3715,  0.3509, -0.1515,  0.0447,  0.1342,\n",
       "          0.3677,  0.5179, -0.5440,  0.2223,  0.1782,  0.6553, -0.1591,  0.0101,\n",
       "         -0.0176,  0.1313, -0.4974,  0.5783, -0.2171, -0.1124, -0.5484, -0.6421,\n",
       "          0.2231, -0.2881, -0.4699, -0.0294, -0.2904,  0.0722, -0.2899,  0.4874,\n",
       "          0.2892,  0.4108, -0.2286,  0.1201,  0.1087, -0.0586, -0.2182,  0.0404,\n",
       "         -0.4564, -0.1814,  0.1302, -0.4615,  0.4367,  0.0845, -0.5777,  0.1769,\n",
       "         -0.0140,  0.0427,  0.0379,  0.0253,  0.7221,  0.4631, -0.1402, -0.8026,\n",
       "          0.4313,  0.2072, -0.6968,  0.1046,  0.0069,  0.3584, -0.1209,  0.0765,\n",
       "         -0.4060,  0.2188, -0.4560, -0.1932, -0.3899,  0.0207,  0.1750, -0.6605,\n",
       "          0.4231,  0.1385,  0.0905, -0.1333,  0.0224, -0.0477, -0.1873,  0.0180,\n",
       "         -0.2859,  0.4312, -0.1609, -0.5253, -0.1304, -0.3109, -0.4464,  0.2957,\n",
       "          0.4460, -0.3395, -0.3715, -0.1733,  0.0156, -0.3417,  0.4977, -0.0394,\n",
       "         -0.1672,  0.0122,  0.1109, -0.3755,  0.0155, -0.1187, -0.3925, -0.0426,\n",
       "          0.3989, -0.2655,  0.0703, -0.0537, -0.5172, -0.0961,  0.1723, -0.1340,\n",
       "         -0.1816, -0.3175, -0.3977,  0.7091, -0.1332, -0.5043, -0.0521, -0.4995,\n",
       "          0.0687, -0.6361,  0.0213, -0.3633, -0.5708, -0.3053]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = CBoW(len(cbow_dataset.vocab), 10)\n",
    "print(m(cbow_dataset[40][0]).shape)\n",
    "m(cbow_dataset[40][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a02fb1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b7c9bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 4.860497951507568\n",
      "epoch: 50, loss: 3.501563549041748\n",
      "epoch: 100, loss: 3.432368040084839\n",
      "epoch: 150, loss: 3.2841267585754395\n",
      "epoch: 200, loss: 2.3874926567077637\n",
      "epoch: 250, loss: 2.4139373302459717\n",
      "epoch: 300, loss: 2.013295888900757\n",
      "epoch: 350, loss: 1.8555536270141602\n",
      "epoch: 400, loss: 1.5092122554779053\n",
      "epoch: 450, loss: 1.6561615467071533\n",
      "epoch: 500, loss: 1.4356046915054321\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 500\n",
    "batch_size = 30\n",
    "learning_rate = 0.0001\n",
    "\n",
    "cbow_dataset = CBoWDataset(raw_corpus)\n",
    "model = CBoW(len(cbow_dataset.vocab), 100)\n",
    "cbow_torch_dataset = cbow_dataset.get_pytorch_dataset()\n",
    "dataloader = DataLoader(cbow_torch_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(0, max_epochs+1):\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        _context, _target = data\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(_context.squeeze(0))\n",
    "        loss = loss_fn(pred, _target.squeeze(0))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"epoch: {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085dd04",
   "metadata": {},
   "source": [
    "### Retrieving embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "494e0dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.7227e-01, -1.4642e+00,  3.3227e-01, -1.1976e+00,  9.9508e-01,\n",
       "        -1.9185e+00,  1.6873e+00, -2.1981e-01, -1.3914e+00, -4.5306e-01,\n",
       "        -8.8307e-01,  3.6806e-01, -1.0049e-01, -5.1561e-01,  5.3933e-01,\n",
       "         1.7580e+00, -1.6260e+00,  9.0106e-02, -1.4081e+00,  5.6344e-01,\n",
       "        -4.0096e-02, -1.0726e-01,  1.6529e+00,  1.9768e+00, -2.3835e+00,\n",
       "        -1.7071e+00, -1.3035e-01,  6.0507e-01,  8.0239e-02,  7.6443e-01,\n",
       "        -7.3667e-01,  1.1619e+00, -5.1986e-01,  8.4223e-01, -1.2132e+00,\n",
       "         1.3615e+00, -5.3674e-01,  2.5407e-01, -2.1256e+00, -2.0824e+00,\n",
       "        -1.2459e+00,  1.2917e-01,  7.6094e-01,  6.7789e-01, -5.5524e-01,\n",
       "        -9.8033e-02, -2.5199e-01,  8.1439e-01, -1.3252e-04,  4.1599e-01,\n",
       "        -8.0540e-01, -7.4667e-01,  1.0687e+00,  2.8368e-01,  7.0659e-01,\n",
       "        -3.4302e-01,  2.9099e-01,  9.6474e-01,  1.6937e+00,  8.2804e-01,\n",
       "         4.1342e-01, -6.6189e-01, -8.8142e-01, -1.4374e+00, -9.3793e-01,\n",
       "         1.5646e+00, -1.1017e+00,  2.0329e+00,  1.1362e+00, -5.9485e-01,\n",
       "         3.6006e-01, -5.4440e-02,  4.5395e-01, -7.3169e-01, -3.6219e-01,\n",
       "         7.0236e-01,  1.4910e+00,  1.7406e+00,  1.4210e+00, -5.3097e-02,\n",
       "         7.2951e-01,  1.1858e+00, -1.1852e+00,  1.5294e+00,  1.0080e-01,\n",
       "        -1.5411e-01,  5.9227e-01, -1.8595e+00, -1.2088e+00,  5.2114e-01,\n",
       "         1.1713e+00,  1.9814e+00, -1.3026e+00,  4.0130e-01,  2.0233e+00,\n",
       "         3.8790e-01, -1.0439e-01, -1.9702e+00, -2.3594e+00, -1.9271e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cbow_embedding(word: str, corpus: CBoWDataset = cbow_dataset) -> torch.Tensor:\n",
    "    try:\n",
    "        idx = corpus.word_2_index[word]\n",
    "    except KeyError:\n",
    "        raise Exception(f\"{word} is not part of the vocabulary\")\n",
    "    else:\n",
    "        embedding = model.embeddings.weight.data[idx]\n",
    "\n",
    "    return embedding\n",
    "\n",
    "get_cbow_embedding(\"this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6255d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
